{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EEG_Eye_State.csv')\n",
    "X = data.loc[:,data.columns !='Eye_detection']\n",
    "y = data['Eye_detection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our RNG seed for reproducibility.\n",
    "RANDOM_STATE_SEED = 123\n",
    "np.random.seed(RANDOM_STATE_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate our examples for our labeled dataset.\n",
    "n_labeled_examples = X.shape[0]\n",
    "training_indices = np.random.randint(low=0, high=n_labeled_examples + 1, size=3000)\n",
    "\n",
    "X_train = X[training_indices]\n",
    "y_train = y[training_indices]\n",
    "\n",
    "# Isolate the non-training examples we'll be querying.\n",
    "X_pool = np.delete(X, (training_indices), axis=0)\n",
    "y_pool = np.delete(y, (training_indices), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12262, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling,entropy_sampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Specify our core estimator along with it's active learning model.\n",
    "\n",
    "learner_us = ActiveLearner(estimator=RandomForestClassifier(), query_strategy=uncertainty_sampling, X_training=X_train, y_training=y_train)\n",
    "learner_es = ActiveLearner(estimator=RandomForestClassifier(), query_strategy=entropy_sampling, X_training=X_train, y_training=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the data we'll need for plotting.\n",
    "predictions_us = learner_us.predict(X)\n",
    "is_correct = (predictions_us == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_es = learner_es.predict(X)\n",
    "is_correct = (predictions_es == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record our learner's score on the raw data.\n",
    "unqueried_score_us = learner_us.score(X, y)\n",
    "unqueried_score_es = learner_es.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Learner Uncertainity Sampling class predictions (Accuracy: 0.897)\n",
      "Active Learner Entropy Sampling class predictions (Accuracy: 0.894)\n"
     ]
    }
   ],
   "source": [
    "print(\"Active Learner Uncertainity Sampling class predictions (Accuracy: {score:.3f})\".format(score=unqueried_score_us))\n",
    "print(\"Active Learner Entropy Sampling class predictions (Accuracy: {score:.3f})\".format(score=unqueried_score_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainity Sampling\n",
      "Accuracy after query 1: 0.8933\n",
      "Accuracy after query 2: 0.8947\n",
      "Accuracy after query 3: 0.8948\n",
      "Accuracy after query 4: 0.8942\n",
      "Accuracy after query 5: 0.8959\n",
      "Accuracy after query 6: 0.8953\n",
      "Accuracy after query 7: 0.8965\n",
      "Accuracy after query 8: 0.8943\n",
      "Accuracy after query 9: 0.8973\n",
      "Accuracy after query 10: 0.8977\n",
      "Accuracy after query 11: 0.8969\n",
      "Accuracy after query 12: 0.8997\n",
      "Accuracy after query 13: 0.8987\n",
      "Accuracy after query 14: 0.8984\n",
      "Accuracy after query 15: 0.8960\n",
      "Accuracy after query 16: 0.9003\n",
      "Accuracy after query 17: 0.8968\n",
      "Accuracy after query 18: 0.8993\n",
      "Accuracy after query 19: 0.8954\n",
      "Accuracy after query 20: 0.8985\n"
     ]
    }
   ],
   "source": [
    "N_QUERIES = 20\n",
    "performance_history = [unqueried_score_us]\n",
    "print(\"Uncertainity Sampling\")\n",
    "# Allow our model to query our unlabeled dataset for the most\n",
    "# informative points according to our query strategy (uncertainty sampling).\n",
    "for index in range(N_QUERIES):\n",
    "  query_index, query_instance = learner_us.query(X_pool)\n",
    "\n",
    "  # Teach our ActiveLearner model the record it has requested.\n",
    "  X1, y1 = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )\n",
    "  learner_us.teach(X=X1, y=y1)\n",
    "\n",
    "  # Remove the queried instance from the unlabeled pool.\n",
    "  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)\n",
    "\n",
    "  # Calculate and report our model's accuracy.\n",
    "  model_accuracy = learner_us.score(X, y)\n",
    "  print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "\n",
    "  # Save our model's performance for plotting.\n",
    "  performance_history.append(model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Sampling\n",
      "Accuracy after query 1: 0.8985\n",
      "Accuracy after query 2: 0.8977\n",
      "Accuracy after query 3: 0.8981\n",
      "Accuracy after query 4: 0.9001\n",
      "Accuracy after query 5: 0.9001\n",
      "Accuracy after query 6: 0.9021\n",
      "Accuracy after query 7: 0.9031\n",
      "Accuracy after query 8: 0.8995\n",
      "Accuracy after query 9: 0.8996\n",
      "Accuracy after query 10: 0.8990\n",
      "Accuracy after query 11: 0.8999\n",
      "Accuracy after query 12: 0.8979\n",
      "Accuracy after query 13: 0.8977\n",
      "Accuracy after query 14: 0.8991\n",
      "Accuracy after query 15: 0.8991\n",
      "Accuracy after query 16: 0.8986\n",
      "Accuracy after query 17: 0.9023\n",
      "Accuracy after query 18: 0.9019\n",
      "Accuracy after query 19: 0.8987\n",
      "Accuracy after query 20: 0.9016\n"
     ]
    }
   ],
   "source": [
    "N_QUERIES = 20\n",
    "performance_history = [unqueried_score_es]\n",
    "print(\"Entropy Sampling\")\n",
    "# Allow our model to query our unlabeled dataset for the most\n",
    "# informative points according to our query strategy (uncertainty sampling).\n",
    "for index in range(N_QUERIES):\n",
    "  query_index, query_instance = learner_es.query(X_pool)\n",
    "\n",
    "  # Teach our ActiveLearner model the record it has requested.\n",
    "  X1, y1 = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )\n",
    "  learner_es.teach(X=X1, y=y1)\n",
    "\n",
    "  # Remove the queried instance from the unlabeled pool.\n",
    "  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)\n",
    "\n",
    "  # Calculate and report our model's accuracy.\n",
    "  model_accuracy = learner_es.score(X, y)\n",
    "  print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "\n",
    "  # Save our model's performance for plotting.\n",
    "  performance_history.append(model_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
